# -*- coding: utf-8 -*-
"""Apollo_TowerRisk.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1YJGVMJBPRq0co3Z1fXzL-b1-iIcgpwIQ
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt


#read training data
from google.colab import files
uploaded = files.upload()

data = pd.read_csv("ADatano20172018.csv")

print(data.columns.values)
datauseful = data.drop(columns = ["SiteCode","HasSolar","Major","FindingDate","Supplier", "MGCluster","OnAirMonths","CurrentStatus","FindingMonth","FindingYear"])

datauseful.isna().sum()
datauseful=datauseful.dropna()
print(datauseful.columns.values)
datauseful.dtypes


#trim strings
datauseful["UrbanRural"]=datauseful["UrbanRural"].str.strip()
datauseful["SiteClass"]=datauseful["SiteClass"].str.strip()
datauseful["AccessHub"]=datauseful["AccessHub"].str.strip()
datauseful["SiteType"]=datauseful["SiteType"].str.strip()
datauseful["StateRegion"]=datauseful["StateRegion"].str.strip()


#regroup

datauseful["UrbanRural"] = datauseful["UrbanRural"].str.replace(".*urban.*" , "Urban", case=False)
datauseful["UrbanRural"] = datauseful["UrbanRural"].str.replace(".*fill.*" , "Urban", case=False)
datauseful.loc[datauseful["UrbanRural"]!="Urban","UrbanRural"]="Rural"
datauseful["UrbanRural"].value_counts()


datauseful.loc[datauseful["SiteType"]=="Monopole","SiteType"]="GBT"
datauseful["SiteType"].value_counts()


datauseful.loc[datauseful["SiteClass"]!="Critical","SiteClass"]="Non-Critical"
datauseful["SiteClass"].value_counts()

datauseful.loc[datauseful["AccessHub"]!="Access","AccessHub"]="Hub"
datauseful["AccessHub"].value_counts()

datauseful["StateRegion"].value_counts()
datauseful.loc[(datauseful["StateRegion"]=="Kachin") | 
                (datauseful["StateRegion"]=="Rakhine") | 
                (datauseful["StateRegion"]=="Sagaing") | 
                (datauseful["StateRegion"]=="Kayah") | 
                (datauseful["StateRegion"]=="Shan"),
               "StateRegion"]="Other"
datauseful["StateRegion"].value_counts()

datauseful = datauseful.drop(columns = ["SiteType"])



#na check
datauseful.isna().sum()


#onehot encoding
ColumnsToEncode = ["UrbanRural","SiteClass","AccessHub","StateRegion"]
for currentcol in ColumnsToEncode:
    dummy1 = pd.get_dummies(datauseful[currentcol],drop_first=False)
    datauseful = pd.concat([datauseful,dummy1],axis=1).drop([currentcol],axis=1)
    


#separate x, y

y = datauseful["MajorMinor"]
y = np.where(y==True,1,0)
x = datauseful.drop(["MajorMinor"],axis=1)
x = x.astype(float)

#split x y
from sklearn.model_selection import train_test_split
x_train, x_test, y_train, y_test = train_test_split(x,y,test_size = 0.2,random_state=123)

#read tower data for prediction

from google.colab import files
uploaded = files.upload()

datapred = pd.read_csv("AAllTowers.csv")

print(datapred.columns.values)
datapreduseful = datapred.drop(columns = ["SiteType","ActualOnAir","PowerModel","Tenants","Supplier"])


datapreduseful.isna().sum()
datapreduseful=datapreduseful.dropna()

print(datauseful.columns.values)
print(datapreduseful.columns.values)


#trim strings
datapreduseful["UrbanRural"]=datapreduseful["UrbanRural"].str.strip()
datapreduseful["SiteClass"]=datapreduseful["SiteClass"].str.strip()
datapreduseful["AccessHub"]=datapreduseful["AccessHub"].str.strip()
#datapreduseful["SiteType"]=datapreduseful["SiteType"].str.strip()
datapreduseful["StateRegion"]=datapreduseful["StateRegion"].str.strip()


#regroup

datapreduseful["UrbanRural"] = datapreduseful["UrbanRural"].str.replace(".*urban.*" , "Urban", case=False)
datapreduseful["UrbanRural"] = datapreduseful["UrbanRural"].str.replace(".*fill.*" , "Urban", case=False)
datapreduseful.loc[datapreduseful["UrbanRural"]!="Urban","UrbanRural"]="Rural"
datapreduseful["UrbanRural"].value_counts()

datapreduseful.loc[datapreduseful["SiteClass"]!="Critical","SiteClass"]="Non-Critical"
datapreduseful["SiteClass"].value_counts()

datapreduseful.loc[datapreduseful["AccessHub"]!="Access","AccessHub"]="Hub"
datapreduseful["AccessHub"].value_counts()

datapreduseful["StateRegion"].value_counts()
datapreduseful.loc[(datapreduseful["StateRegion"]=="Kachin") | 
                (datapreduseful["StateRegion"]=="Rakhine") | 
                (datapreduseful["StateRegion"]=="Sagaing") | 
                (datapreduseful["StateRegion"]=="Kayah") | 
                (datapreduseful["StateRegion"]=="Shan") |
                (datapreduseful["StateRegion"]=="Tanintharyi")|
                (datapreduseful["StateRegion"]=="Magway"),
               "StateRegion"]="Other"
datapreduseful["StateRegion"].value_counts()


#na check
datapreduseful.isna().sum()



#onehot encoding
ColumnsToEncode = ["UrbanRural","SiteClass","AccessHub","StateRegion"]
for currentcol in ColumnsToEncode:
    dummy1 = pd.get_dummies(datapreduseful[currentcol],drop_first=False)
    datapreduseful = pd.concat([datapreduseful,dummy1],axis=1).drop([currentcol],axis=1)
    

alltowers_rowlabelcolumns = datapreduseful[["OurSitecode","VendorSitecode"]] 
x_alltowers = datapreduseful.drop(columns=["OurSitecode","VendorSitecode"]).astype(float)
x_alltowers.isna().sum()

#Logistic

#build
import statsmodels.api as sm
sm_fitted_logit = sm.Logit(y_train, sm.add_constant(x_train,has_constant="add"))
sm_fitted_logit = sm_fitted_logit.fit()
print(sm_fitted_logit.summary2())


#predict with test dataset
y_pred2=sm_fitted_logit.predict(sm.add_constant(x_test,has_constant="add"))

from sklearn import metrics
fpr,tpr,thresholds = metrics.roc_curve(y_test,y_pred2)
plt.plot(fpr,tpr,color='red')
auc2 = metrics.roc_auc_score(y_test,y_pred2)
print(auc2)

#predict to all towers
y_pred2_alltowers = sm_fitted_logit.predict(sm.add_constant(x_alltowers,has_constant="add"))
y_pred2_alltowerscomplete = pd.concat([alltowers_rowlabelcolumns,y_pred2_alltowers],axis=1)
y_pred2_alltowerscomplete.to_csv("Apollo_Logistic.csv")

#XGBoost
params = {'learning_rate': [0.01,0.05,0.1,0.2,0.3],
          'max_depth':[2,3,4,5,6],
          'min_child_weight':[2,3,4,5],
          'n_estimators':[100,250,500,1000]}


from xgboost import XGBClassifier
model=XGBClassifier(learning_rate=0.3,n_estimators=3000,min_child_weight=3,max_depth = 4,colsample_bytree=1,use_label_encoder=False,eval_metric="auc",objective="binary:logistic")


from sklearn.model_selection import GridSearchCV
grid_search= GridSearchCV(model,param_grid=params,cv=5,scoring='roc_auc',n_jobs=-1)
grid_search.fit(x_train,y_train)

print(grid_search.best_params_)
print(grid_search.best_score_)
print(grid_search.n_splits_)
print(grid_search.scorer_)
print(grid_search.best_estimator_)

model=grid_search.best_estimator_
model.fit(x_train,y_train)

#predict with test dataset
y_pred5 =  model.predict_proba(x_test)[:,1]

from sklearn import metrics
fpr,tpr,thresholds = metrics.roc_curve(y_test,y_pred5)
plt.plot(fpr,tpr,color='red')
auc5 = metrics.roc_auc_score(y_test,y_pred5)
print(auc5)

#explain
from xgboost import plot_importance
plot_importance(model)

!pip install shap
import shap
shap_values = shap.TreeExplainer(model).shap_values(x_test)
shap.summary_plot(shap_values, x_test)

#predict to all towers
y_pred5_alltowers = model.predict_proba(x_alltowers)[:,1]
y_pred5_alltowers = pd.DataFrame(y_pred5_alltowers,columns=["XGB"])

y_pred5_alltowerscomplete = pd.concat([alltowers_rowlabelcolumns.reset_index(drop=True),y_pred5_alltowers.reset_index(drop=True)],axis=1)

y_pred5_alltowerscomplete.to_csv("Apollo_XGB.csv")

#CatBoost hypertuning GridSearch
params=""
model=""

params = {'learning_rate': [0.005,0.01,0.025,0.05,0.075],
          'depth':[2,3,4,5,6],
          'iterations':[1000,2000]}

!pip install catboost
from catboost import CatBoostClassifier, Pool

train_pool=Pool(x_train, y_train)
model = CatBoostClassifier(iterations=100,
                           depth=2,
                           learning_rate=0.01,
                           loss_function='Logloss',
                           verbose=False,
                           eval_metric='AUC')

from sklearn.model_selection import GridSearchCV
grid_search= GridSearchCV(model,param_grid=params,cv=5,scoring='roc_auc',n_jobs=-1)
grid_search.fit(x_train,y_train)

print(grid_search.best_params_)
print(grid_search.best_score_)
print(grid_search.n_splits_)
print(grid_search.scorer_)
print(grid_search.best_estimator_)

model=grid_search.best_estimator_
model.fit(train_pool)


#predict with test dataset
test_pool = Pool(x_test,y_test)
y_pred8 = model.predict_proba(test_pool)[:,1]

from sklearn import metrics
fpr,tpr,thresholds = metrics.roc_curve(y_test,y_pred8)
plt.plot(fpr,tpr,color='red')
auc8 = metrics.roc_auc_score(y_test,y_pred8)
print(auc8)

#explain
shap_values = model.get_feature_importance(test_pool,type="ShapValues")
shap.summary_plot(shap_values[:,:-1], x_test)


#predict to all towers
y_pred8_alltowers = model.predict_proba(x_alltowers)[:,1]
y_pred8_alltowers = pd.DataFrame(y_pred8_alltowers,columns=["Catboost"])

y_pred8_alltowerscomplete = pd.concat([alltowers_rowlabelcolumns.reset_index(drop=True),y_pred8_alltowers.reset_index(drop=True)],axis=1)

y_pred8_alltowerscomplete.to_csv("Apollo_Catboost.csv")

#Neural networks

import tensorflow as tf
import keras
from sklearn.preprocessing import scale
from sklearn.preprocessing import MinMaxScaler
from sklearn.model_selection import train_test_split
from tensorflow.keras.optimizers import Adam

#check GPU
from tensorflow.python.client import device_lib
print(device_lib.list_local_devices())

#scale before splitting
scaler = MinMaxScaler(copy=False)
scaler.fit(x)
x_scaled = scaler.transform(x)
#x_scaled = scale(x)
x_train_scaled, x_test_scaled, y_train, y_test = train_test_split(x_scaled,y,test_size = 0.2,random_state=123)

#build model shell

def model_builder(hp_units,do_rate):

  model=tf.keras.models.Sequential()
  model.add(keras.layers.Flatten(input_dim=x_train_scaled.shape[1]))

  model.add(tf.keras.layers.Dropout(rate = do_rate))  
  model.add(tf.keras.layers.Dense(hp_units,activation='sigmoid'))
  
  model.add(tf.keras.layers.Dropout(rate = do_rate))
  model.add(tf.keras.layers.Dense(hp_units,activation='sigmoid'))
  
  model.add(tf.keras.layers.Dropout(rate = do_rate))
  model.add(tf.keras.layers.Dense(hp_units,activation='sigmoid'))

  model.add(tf.keras.layers.Dense(1,activation='sigmoid'))

  model.compile(optimizer="Adam",
                  loss='binary_crossentropy',
                  metrics=['accuracy',tf.keras.metrics.AUC()])
  return model

model = model_builder(32,0.2)

history = model.fit(x_train_scaled, y_train, epochs=1000, validation_data=(x_test_scaled,y_test),verbose=0)


#print(model.summary())
currentaucstring = history.history.keys()
currentaucstring = list(currentaucstring)[2]
currentvalstring = "val_"+currentaucstring
#print(history.history.keys())

val_auc_per_epoch = history.history[currentvalstring]
best_epoch = val_auc_per_epoch.index(max(val_auc_per_epoch)) + 1
print('Best epoch:', best_epoch, "; Max test AUC: ",max(val_auc_per_epoch))

# summarize epoch history for auc
plt.plot(history.history[currentaucstring])
plt.plot(history.history[currentvalstring])
plt.title('model auc')
plt.ylabel('auc')
plt.xlabel('epoch')
plt.legend(['train', 'test'], loc='upper left')
plt.show()

# Retrain the model
#best_epoch=450
model.fit(x_train_scaled, y_train, epochs=best_epoch, validation_data=(x_test_scaled,y_test),verbose=0)

#Predict the response for test dataset
y_pred13= model.predict(x_test_scaled)


from sklearn import metrics
fpr,tpr,thresholds = metrics.roc_curve(y_test,y_pred13)
plt.plot(fpr,tpr,color='red')
auc13 = metrics.roc_auc_score(y_test,y_pred13)
print(auc13)

#shap values
!pip install shap
import shap
def f_wrapper(X):
  return model.predict(X).flatten()

columnlist=x_train.columns.tolist()
#x_train_sample=shap.sample(x_train_scaled,100)
explainer=shap.KernelExplainer(f_wrapper,x_train_scaled)
shap_values = explainer.shap_values(x_test_scaled,nsamples=50)
shap.summary_plot(shap_values, x_test_scaled,feature_names=columnlist)

#predict to all towers

x_alltowers_scaled = scale(x_alltowers)

y_pred13_alltowers = model.predict(x_alltowers_scaled)
y_pred13_alltowers = pd.DataFrame(y_pred13_alltowers,columns=["NN"])

y_pred13_alltowerscomplete = pd.concat([alltowers_rowlabelcolumns.reset_index(drop=True),y_pred13_alltowers.reset_index(drop=True)],axis=1)

y_pred13_alltowerscomplete.to_csv("Apollo_NN.csv")